---
tags:
- editors
---

## **Bolt.new

The video introduces a fork of the Bolt open-source project that can be used locally, with the added benefit of integration with multiple free API providers. Unlike Bolt, which requires Anthropic API keys, this version works with providers like Gemini, Mistral, and even supports local models like Ollama. The main highlights include the ability to publish directly to GitHub, sync files to a local folder, and a variety of integrations like DeepSeek and OpenRouter. Below, we'll go over how to install and set up this tool.

### Features of the Bolt Fork

1. **Multiple Provider Integrations**: Supports Gemini, OpenRouter, Anthropic, Mistral, and local Ollama models.
2. **Enhanced System Prompt and API Integration**: Includes integration with DeepSeek API, Mistral API, and OpenAI-compatible APIs.
3. **Sync and GitHub Publishing**: Syncs files to a local folder and directly publishes projects to GitHub.
4. **Code Preview with Web Containers**: The tool uses web containers, which provide an in-browser virtual machine environment for code preview and interaction.
5. **Additional Commands**: Allows code downloading, syncing, toggling the terminal, and pushing to GitHub from the interface.

### Comparison to [[VSCode]]

The Bolt fork and VSCode have some overlapping features but also differ significantly in their approach and capabilities:

1. **Integration with Multiple Providers**: Unlike VSCode, which relies on extensions for integration with different API providers, the Bolt fork has built-in support for multiple providers like Gemini, Mistral, and Ollama. VSCode's extensibility is broader, allowing users to integrate various languages and services through a wide array of plugins.
2. **Setup Complexity**: Setting up the Bolt fork requires cloning the repository, installing dependencies, and configuring environment files, which might be more complex compared to installing VSCode and adding extensions. VSCode offers a more user-friendly setup experience, especially for beginners.
3. **Code Preview**: Bolt uses web containers to provide an in-browser virtual machine environment, enabling users to preview and interact with generated code directly. VSCode, on the other hand, allows code execution through integrated terminals and extensions, but it does not provide a built-in VM environment like Bolt's web containers.
4. **Sync and [[GitHub]] Integration**: While both tools offer GitHub integration, Bolt allows direct publishing to GitHub and syncing files to a local folder automatically. VSCode provides similar features through its Git integration, but users need to manually configure and use Git commands or extensions to achieve the same level of automation.
5. **Terminal Access**: Bolt's interface includes a terminal feature for executing commands, similar to VSCode's integrated terminal. However, VSCode's terminal is more versatile, supporting multiple shells and offering a more integrated development environment.
6. **Usage Flexibility**: Bolt is tailored for generating and previewing code using specific models and APIs, while VSCode is a general-purpose code editor that supports a wide range of programming languages, debugging tools, and extensions for different development workflows.

### Integration with VSCode

Bolt.new can be integrated with VSCode to some extent. Here are some potential ways to integrate them:

1. **Using VSCode Extensions**: You can use VSCode's REST Client or HTTP extensions to make API calls to Bolt's backend, allowing you to interact with Bolt from within VSCode. This can streamline the process of generating code and accessing models directly from your VSCode editor.
2. **File Syncing**: Since Bolt supports syncing files to a local folder, you can edit those synced files directly in VSCode. This allows you to use VSCode's full suite of editing tools, debugging, and extensions to further develop or modify code generated by Bolt.
3. **Terminal Commands**: You can also run Bolt's commands (e.g., starting the server) from VSCode's integrated terminal. This lets you manage Bolt without leaving your preferred code editor.
4. **Git Integration**: By using Bolt's GitHub publishing feature, you can push generated projects to GitHub, which can then be cloned and edited in VSCode, allowing a seamless workflow between the two tools.

### Installation and Setup

1. **Clone the Repository**:

    ```
    git clone <repository_url>
    cd <repository_folder>
    ```

2. **Install Dependencies**:

    ```
    pnpm install
    ```

    This command installs all the required dependencies.

3. **Rename the Environment File**:

    ```
    mv .env.example .env.local
    ```

    Modify the `.env.local` file to include your API keys for providers you want to use, such as Gemini, Mistral, OpenAI, etc.

4. **Running the Tool**:

    ```
    pnpm preview
    ```

    This command starts the application locally. The output will show the local server address, such as `localhost:<port_number>`, which you can open in your browser.

### Usage

- Once the application is running, you can select your provider (e.g., Gemini, Mistral) and generate code by interacting with the interface.
- The interface includes:
    - **Providers Section**: Choose from multiple model providers, such as Mistral for local and free API usage.
    - **Download and Sync Options**: Download generated code as a ZIP or sync it to a local folder.
    - **Push to GitHub**: You can also directly push your generated projects to a GitHub repository.
    - **Terminal Access**: The interface has a terminal feature where commands can be executed directly.

### How to Use GitHub Models Free API

- To use GitHub Models with GPT-4:

    1. **Get a GitHub Models API Key**: Go to GitHub Models, create an API key, and use it to access models like GPT-4 for free.
    2. **Set Up Light LLM Proxy**:

        - Install Light LLM using:

            ```
            npm install -g light-llm
            ```

        - Export the GitHub Models API key and start the proxy server:

            ```
            export GITHUB_MODELS_API_KEY=<your_api_key>
            light-llm start
            ```

        - Copy the proxy server URL and paste it in the `.env.local` file under the OpenAI-compatible base URL.

### Additional Options

- Generate projects (like a Minesweeper game) using GitHub Models API via the configured proxy server.
- Utilize free APIs for unrestricted local usage without rate limits (e.g., Mistral).

Overall, this fork of Bolt provides a powerful, flexible, and open-source alternative that enables local AI model usage with various free API integrations, providing an experience similar to more restricted tools like v0 but without associated costs.

[[Python]]